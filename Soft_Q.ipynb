{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc4503f-ec75-46e5-a550-127c823e9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter estimation of \n",
    "# alpha (learning rate)\n",
    "# beta  (inverse temperature)\n",
    "# in softmax Q-learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252ad561-e5f4-4bae-9725-9a1a9b1cb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from scipy import optimize as optim\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c4b747-9d1f-4881-b943-d8791a37bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 10\n",
    "N_TRIALS = 40\n",
    "VARS  = np.array([2, 2])\n",
    "MEAN_LS = np.array([(1,-1), (-1,1), (0,0)])\n",
    "ALPHA, BETA = (0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b336f0-d5c9-4972-817d-049a1ca434fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Soft_Q_learners():\n",
    "    \"\"\"Softmax Q-learning agents for 2-armed bandit\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, beta, n_sample=N_SAMPLES):\n",
    "        \n",
    "        self.n = n_sample\n",
    "        self.Q_tables = np.zeros((n_sample, 2))\n",
    "        self.alpha = alpha\n",
    "        self.beta  = beta\n",
    "        \n",
    "    def act(self):\n",
    "        probs = softmax(self.Q_tables, axis=-1)\n",
    "        actions = probs[:,0] > np.random.rand(self.n)\n",
    "        return actions.reshape(-1,1)\n",
    "    \n",
    "    def learn(self, actions, rewards):\n",
    "        self.Q_tables[range(self.n), actions] += self.alpha * (rewards - self.Q_tables[range(self.n), actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e0a7acb-aff5-4588-9b00-5c0e10fb8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 2-armed bandit games\n",
    "\n",
    "choice_ls = []\n",
    "reward_ls = []\n",
    "Qvalue_ls = []\n",
    "\n",
    "agents = Soft_Q_learners(alpha=ALPHA, beta=BETA)\n",
    "for _ in range(10):\n",
    "    \n",
    "    MEANS = MEAN_LS[np.random.choice(range(len(MEAN_LS)))]\n",
    "    \n",
    "    for _ in range(N_TRIALS):\n",
    "        actions = np.logical_not(agents.act()[:,0]).astype(int)\n",
    "        rewards = MEANS[actions] + np.random.randn(N_SAMPLES) * VARS[actions]\n",
    "        agents.learn(actions, rewards)\n",
    "\n",
    "        choice_ls.append(actions)\n",
    "        reward_ls.append(rewards)\n",
    "        Qvalue_ls.append(np.copy(agents.Q_tables))\n",
    "    \n",
    "choice_ls = np.array(choice_ls).T\n",
    "reward_ls = np.array(reward_ls).T\n",
    "Qvalue_ls = np.array(Qvalue_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f58638f4-1a7c-44cf-b1d1-2d18c50d9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(param, choices, rewards):\n",
    "    \n",
    "    LL = 0\n",
    "    Q = np.array([0.,0.])\n",
    "    \n",
    "    # map range of alpha to (0, 1); beta to (0, inf)\n",
    "    alpha = 1 / (1 + np.exp(-param[0]))\n",
    "    beta = np.exp(param[1])\n",
    "    \n",
    "    for choice, reward in zip(choices, rewards):\n",
    "        \n",
    "        # make prediction\n",
    "        probs = softmax(beta * Q)\n",
    "        LL -= np.log(probs[choice] + 1e-8)\n",
    "        \n",
    "        # update Q-value\n",
    "        Q_err = reward - Q[choice]\n",
    "        Q[choice] += alpha * Q_err\n",
    "        \n",
    "    return LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6330d802-2f03-4cb4-b623-4269beac07c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALPHA:0.1   BETA:1\n",
      "\n",
      "Sample0:    alpha:0.0886    beta:1.0593    iters:09\n",
      "Sample1:    alpha:0.1084    beta:0.7843    iters:10\n",
      "Sample2:    alpha:0.0867    beta:0.8635    iters:10\n",
      "Sample3:    alpha:0.0922    beta:1.1205    iters:10\n",
      "Sample4:    alpha:0.1139    beta:0.8045    iters:10\n",
      "Sample5:    alpha:0.0856    beta:0.9799    iters:08\n",
      "Sample6:    alpha:0.1136    beta:0.9014    iters:09\n",
      "Sample7:    alpha:0.0983    beta:0.9181    iters:10\n",
      "Sample8:    alpha:0.1307    beta:0.7285    iters:09\n",
      "Sample9:    alpha:0.0772    beta:1.1619    iters:07\n"
     ]
    }
   ],
   "source": [
    "print(f\"ALPHA:{ALPHA}   BETA:{BETA}\\n\")\n",
    "\n",
    "# estimate parameters\n",
    "\n",
    "X0 = np.array([0, 0])  # initial guess\n",
    "\n",
    "for i in range(N_SAMPLES):\n",
    "    \n",
    "    # compute mle\n",
    "    x = optim.minimize(\n",
    "        fun=likelihood, \n",
    "        x0=X0, \n",
    "        args=(choice_ls[i], reward_ls[i])\n",
    "    )\n",
    "    \n",
    "    # recover parameters\n",
    "    alpha = 1 / (1 + np.exp(-x.x[0]))\n",
    "    beta = np.exp(x.x[1])\n",
    "\n",
    "    print(f\"Sample{i}:    alpha:{alpha:.4f}    beta:{beta:.4f}    iters:{x.nit:02d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2296f86a-81ad-4f5e-95b9-2ed4b23b0944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([-0.37,  0.  ]), array([-0.33,  0.  ])],\n",
       " [array([-0.37, -0.05]), array([-0.33, -0.05])],\n",
       " [array([-0.37,  0.05]), array([-0.33,  0.04])],\n",
       " [array([-0.37,  0.26]), array([-0.33,  0.23])],\n",
       " [array([-0.37,  0.43]), array([-0.33,  0.38])],\n",
       " [array([-0.37,  0.57]), array([-0.33,  0.51])],\n",
       " [array([-0.33,  0.57]), array([-0.3 ,  0.51])],\n",
       " [array([-0.33,  0.77]), array([-0.3 ,  0.69])],\n",
       " [array([-0.19,  0.77]), array([-0.17,  0.69])],\n",
       " [array([-0.43,  0.77]), array([-0.39,  0.69])],\n",
       " [array([-0.43,  1.01]), array([-0.39,  0.91])],\n",
       " [array([-0.43,  1.44]), array([-0.39,  1.3 ])],\n",
       " [array([-0.43,  1.09]), array([-0.39,  1.  ])],\n",
       " [array([-0.57,  1.09]), array([-0.51,  1.  ])],\n",
       " [array([-0.54,  1.09]), array([-0.49,  1.  ])],\n",
       " [array([-0.54,  1.48]), array([-0.49,  1.36])],\n",
       " [array([-0.54,  1.72]), array([-0.49,  1.59])],\n",
       " [array([-0.54,  1.66]), array([-0.49,  1.54])],\n",
       " [array([-0.54,  1.39]), array([-0.49,  1.31])],\n",
       " [array([-0.54,  1.07]), array([-0.49,  1.03])]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# latent variable estimation (Q-value)\n",
    "# compute parameter\n",
    "alpha_, beta_ = optim.minimize(\n",
    "    fun=likelihood, \n",
    "    x0=X0, \n",
    "    args=(choice_ls[0], reward_ls[0])\n",
    ").x\n",
    "alpha_ = 1 / (1 + np.exp(-alpha_))\n",
    "beta_  = np.exp(beta_)\n",
    "\n",
    "# simulate\n",
    "Q_ = [0.,0.]\n",
    "ls = []\n",
    "for i in range(len(choice_ls[0])):\n",
    "    choice = choice_ls[0,i]\n",
    "    reward = reward_ls[0,i]\n",
    "    Q_[choice] += alpha_ * (reward - Q_[choice])\n",
    "    ls.append([Qvalue_ls[i, 0], np.array(Q_)])\n",
    "    \n",
    "ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06606949-e2a7-4c26-a3ff-e047c6ecde5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
